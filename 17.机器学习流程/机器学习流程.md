# 机器学习流程.md

机器学习 (ML) 工作流是代码和示例的集合，旨在通过 Isaac SDK 加速 ML 的采用。 这些示例使用 Tensorflow 框架进行训练，但相同的原则和代码也适用于 PyTorch 等其他 ML 框架。

## 模拟训练
训练数据很难收集，也更难标记。 合并来自仿真的合成数据可以加速该过程。 [自由空间分割部分](https://docs.nvidia.com/isaac/packages/freespace_dnn/doc/freespace_segmentation.html#freespace-segmentation)描述了使用 Unity3D 从合成数据训练样本 ML 模型。

## PC 和边缘设备上的推理
Isaac SDK 中提供了三种可能的运行时，以在 Jetson TX2/Xavier 和 PC 上使用经过训练的 ML 模型进行推理。 所有示例都可以在两个平台上编译和运行。


### 使用 TensorRT 进行推理
[TensorRT](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/index.html) 是 NVIDIA 的深度学习推理优化工具和运行时。 它旨在提供低延迟和高吞吐量。 TensorRT 提供了一个 ONNX 解析器，因此您可以轻松地从所有主要框架导入 ONNX 模型，例如 PyTorch、Caffe 2、Microsoft Cognitive Toolkit、MxNet、Tensorflow、Chainer 等。

有关在 Isaac 中使用 TensorRT 的更多信息，请参阅 [TensorRT Inference Codelet](https://docs.nvidia.com/isaac/packages/ml/ml.html#tensorrt-inference-chapter)、其[ API 引用 isaac.ml.TensorRTInference](https://docs.nvidia.com/isaac/doc/doc/component_api.html#isaac-ml-tensorrtinference) 和 [TensorRT 开发人员指南](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html)。

### Torch推理
[Torch](http://torch.ch/) 是一个广泛支持深度学习算法的科学计算框架。 得益于简单快速的脚本语言 Lua 和底层 C/CUDA 实现，Torch 易于使用且高效。

有关使用 Torch 的更多信息，请参阅 [isaac.ml.TorchInference API](https://docs.nvidia.com/isaac/doc/doc/component_api.html#isaac-ml-torchinference) 参考和 [Torch 文档](http://torch.ch/docs/getting-started.html)。

### 使用 Tensorflow 进行推理
[Tensorflow](https://www.tensorflow.org/) 是来自 Google 的一种流行的 ML 框架，用于在此处提供的示例中进行训练。 Isaac SDK 还与 Tensorflow 运行时一起使用，以按原样对经过训练的模型执行推理。

有关在 Isaac 中使用 Tensorflow 的更多信息，请参阅 [TensorflowInference Codelet](https://docs.nvidia.com/isaac/packages/ml/ml.html#tensorflow-inference-chapter)、其 [API 引用 isaac.ml.TensorflowInference](https://docs.nvidia.com/isaac/doc/doc/component_api.html#isaac-ml-tensorflowinference) 和 [Tensorflow 开发人员指南](https://www.tensorflow.org/guide.html)。


**注意**

Tensorflow 弃用警告。 NVIDIA 正在弃用 Tensorflow 推理支持。 它已经过测试并且可以正常运行，但 NVIDIA 计划在随后的主要版本中删除支持。 对于 Tensorflow 推理，请计划将您的工作流程迁移到 ONNX / TensorRT。


## 示例
开发、训练和调整深度学习模型需要大量数据和计算能力。 此类繁重的任务预计将由云端或计算集群中的存储和 GPU 执行，而真正的机器人应用程序运行在计算能力有限的边缘设备上。

从数据收集一直到将深度学习模型部署到机器人中的顺畅工作流程加速了机器人应用程序的开发。

## PyCodelet
许多 ML 开发人员精通 Python。 PyCodelet 有助于在 Python 中与 Isaac SDK 进行数据传输。

```bash
<IsaacSDK>/apps/packages/pyalice/tests/pycodelet_test.py
```

ML 开发人员可以使用 PyCodelet 在 Python 中编码，而不是在 C++ 中编码 Codelet。 首先，需要将基于 Python 的小码声明为 alice.Codelet 的子类。

```Python
class MyPyCodeletProducer(Codelet):
```

就像基于 C++ 的 Codelet 一样，开发人员需要覆盖 3 个成员函数：`start()`、`tick()` 和 `stop()`。 tick 会在消息到达或超时时定期调用，而 start() 和 stop() 会在 Codelet 进入或退出运行状态时调用。

尽管 Python Codelet 与 C++ Codelet 具有相似的概念，但仍存在一些细微差别：

* Python Codelet 需要通过 isaac_proto_tx() 和 isaac_proto_rx() 显式检索消息的挂钩。

* Python Codelet 需要通过 get_proto() 检索消息并通过 init_proto() 从 hook 中显式创建消息。

* 为了在应用程序中促进 Python Codelet，Python Codelet 的节点是通过 alice.loadGraphFromFile() 使用以下 JSON 规范创建的。

```json
{
  "name": "foo_node",
  "components": [
    {
      "name": "ml",
      "type": "isaac::alice::MessageLedger"
    },
    {
      "name": "isaac.alice.PyCodelet",
      "type": "isaac::alice::PyCodelet"
    }
  ]
}
```
稍后显式调用 `alice.register_pycodelets()` 以使用如下所示的映射将 Python Codelet 绑定到这些节点。

```json
{
  "producer": MyPyCodeletProducer,
  "consumer": MyPyCodeletConsumer
}
```
Python codelets 可以通过 JSON 访问参数。 检查 python_ping 示例。

## 配套代码
### Tensorflow 推理小码
TensorflowInference codelet 采用经过训练的 Tensorflow 冻结模型并在 Isaac SDK 应用程序中运行推理。 输入和输出消息都是 TensorList，它是 TensorProto 消息的列表。 小码采用几个参数，如球分割推理应用程序的配置所示：

```json
"model_file_path": "/tmp/ball_segmentation/ckpts/model-0-frozen.pb",
"config_file_path": "",
"input_tensor_info": [
  {
    "ops_name": "input",
    "index": 0,
    "dims": [1, 256, 512, 3]
  }
],
"output_tensor_info": [
  {
    "ops_name": "output",
    "index": 0,
    "dims": [1, 256, 512, 1]
  }
]
```

* `model_file_path` - 指向要在应用程序中运行的 Tensorflow 冻结模型。 有关冻结模型的更多信息，请参阅 Tensorflow NVIDIA GPU-Accelerated 容器和 Tensorflow 工具。

* `config_file_path` - 指向包含用于配置 Tensorflow 运行时的 Tensorflow ConfigProto 对象的 protobuf 文件。 使用以下命令作为自定义的起点。

    ```bash
    python -c "import tensorflow as tf; f=open('config_proto.pb', 'w');f.write(tf.ConfigProto(allow_soft_placement=True, gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5)).SerializeToString()); f.close()"
    ```

* input_tensor_info - 输入张量规范列表。 每个规范都包括操作名称（如在 NodeDef 中）、操作中输入的索引和张量维度。

    例如，以下代码指定了三个输入张量：

```json
"input_tensor_info": [
  {
    "ops_name": "input_color",
    "dims": [1, 3, 256, 512],
    "index": 0
  },
  {
    "ops_name": "input_color",
    "dims": [1, 3, 256, 512]
    "index": 1
  },
  {
    "ops_name": "input_depth",
    "dims": [1, 1, 256, 512]
    "index": 0
  }
]
```
output_tensor_info - 输出张量规范列表。 每个规范都包括操作名称（如在 NodeDef 中）、操作中输出的索引和张量维度。

例如，以下代码指定操作的输出张量：

```json
"output_tensor_info": [
  {
    "ops_name": "Predictions/Reshape",
    "dims": [1, 1001],
    "index": 0
  }
]
```

### TensorRT 推理小码
TensorRT 推理小代码采用 TensorRT .plan、.uff 或 .onnx 模型，并在 GPU 上的 Isaac SDK 应用程序中运行推理。 输入和输出消息都是TensorList，这是一个TensorProto消息列表。

TensorRT Inference 采用以下参数：

* model_file_path - UFF 或 ONNX 格式的设备不可知模型的路径。 UFF 模型可以通过 UFF 转换器工具创建，或者从 Tensorflow GraphDef、Frozen Protobuf 模型或 Keras 模型转换而来。 使用此工具的示例代码和 Bazel 构建步骤位于 `packages/ml/tools`。 [ONNX 格式](https://onnx.ai/)是一种开放格式，受 Caffe2、MXNet 和 PyTorch 的支持，并且导出到 ONNX 可用于所有主要框架。 另请参阅：[ONNX 教程](https://github.com/onnx/tutorials)。

    **注意**

    如果在默认或指定位置找到特定于设备的序列化引擎文件，则模型文件将被忽略（见下文）。

    **注意**

    UFF 弃用警告。 我们正在从 TensorRT 7 中弃用 Tensorflow 的 UFF Parser。它已经过测试并且可以正常工作，但我们计划在后续的主要版本中删除支持。 请计划将您的工作流程迁移到 ONNX。


* model_file_path（可选）- 特定于设备的序列化 TensorRT 引擎的路径。 引擎可以由 Isaac TensorRT codelet 自动创建，也可以在目标设备上首次启动时从模型创建。 此转换过程和针对目标设备的优化可能需要几秒到几分钟，具体取决于模型的大小和系统的性能。 或者，引擎可以预先缓存在设备上。

    如果未指定此参数，则默认设置为模型文件路径，扩展名替换为 .plan。

    **注意**

    引擎文件（如果存在）优先于模型文件。

    **注意**

    该引擎不能在不同设备、不同版本的 TensorRT 或不同版本的 CuDNN 库之间移植。

* input_tensor_info - 输入张量规范列表。 每个规范都包括操作名称（如在 NodeDef 中）、张量维度和可选参数。

    举个例子，下面的代码指定了两个输入张量：

```json
"input_tensor_info": [
  {
    "operation_name": "input_1",
    "dims": [1, 3, 256, 512]
  },
  {
    "operation_name": "input_2",
    "dims": [1, 3, 256, 512]
  }
]

```

**输入张量规范参数**

  * operation_name - 要在模型图中查找的操作的名称。

  * dims - 张量维度。 注意：在 TensorRT 文档中，各个张量维度可能使用以下名称引用：

```json
"dims": [Batch Size, Channel, Rows, Columns]
```
* Batch Size（可选，默认值 = 1）- 一批中的样本数。

    **注意**

    要指定可变批量大小，请将此参数设置为 -1。 还必须设置最大批量大小参数（见下文）。 ONNX 当前不支持可变批量大小； 计划在后续版本中提供支持。


* 通道 - 图像通道或矢量分量的数量。

* 行数（可选）- 矩阵的行数（或图像的高度）。

* 列（可选）- 矩阵的列数（或图像的宽度）。

例如，假设 Tensorflow/Keras 模型是在 320x200x3 (RGB) 图像上训练的。 这种模型的正确设置如下：

```json
"dims": [1, 3, 200, 320]
```

**注意**

虽然模型训练期间的输入顺序（或权重内存布局）可以使用以下任一格式，但无论原始框架输入顺序如何，此小代码在推理时仅支持输入张量的 channels_first 格式：

* channels_last：以 Channel 作为张量的最后一个索引。 TensorRT 文档将其称为“NHWC”布局（批号、高度、宽度、通道）。 例如，[1, 200, 320, 3]。

* channels_first：以 Channel 作为张量的第一个（或第二个）索引。 TensorRT 文档将其称为“NCHW”布局（批号、通道、高度、宽度）。 例如，[1, 3, 200, 320]。

**模型解析器使用此输入张量规范列表来剪切**

图的一部分用于推理，设置可变大小输入的维度并执行内存分配。 它还在推理时使用，以验证输入张量的等级和大小。

**输入张量规范应符合**：

* Isaac 图中前一个节点的输出。 输入张量规范的张量等级和维度应与前一个节点的输出相匹配。 前一个节点输出的 Batch Size 应该小于或等于引擎的最大 batch size。

    如果未设置批量大小（已弃用），则先前的节点输出也应省略它：

```json
"input_tensor_info": [
  {
    "dims": [3, 256, 512]
...

"isaac.ml.TensorReshape": {
  "output_tensors_dimension": [[3, 256, 512]]
```

* 正在解析的训练模型的规范。 该模型应包含大小固定以匹配输入张量规范或可变大小 (-1) 的匹配节点。

* TensorRT 的局限性。 TensorRT 目前仅支持以下输入内存布局：

    * (Batch Size, Channel, Rows, Columns)，例如 [1, 3, 480, 640]；

    * (Channel, Rows, Columns)，例如[3, 480, 640]；

    * (Batch Size, Channel)，例如[1, 1024]；

    * (Channel)，例如 [1024]。

**注意**

图像的 TensorRT 输入顺序是颜色平面而不是混合颜色通道阵列（即不是 RGB）。 通常，它需要将图像转换为彩色平面格式或通道轴的转置。 这可以通过选择相关的图像编码顺序来实现，例如：
```json
"tensor_encoder": {
  "isaac.ml.ColorCameraEncoder": {
    "rows": 480,
    "cols": 640,
    "pixel_normalization_mode": "PositiveNegative",
    "tensor_index_order": "201"
  }
```

* output_tensor_info - 输出张量规范列表。 每个规范包括操作名称（如在 NodeDef 中）和张量维度。

    例如，以下代码指定一个输出张量：
```json
"output_tensor_info": [
  {
    "operation_name": "output",
    "dims": [1, 1001]
  }
]

```

另请参阅上面的输入张量规范。

* `max_batch_size`（可选）- 将调整引擎的批量大小。 在执行时，可以使用较小的批次，但不能使用更大的批次。 默认设置为“-1”，指定输入张量大小将用于推断最大批量大小。 注意：此参数会影响 GPU 内存分配量和引擎性能。

    输入和输出张量规格应具有相同的批量大小。 此批量大小应小于或等于模型的最大批量大小。

    如果 batch size 等于 1，则可以收回该维度，例如：

    ```json
    "dims": [1, 256, 512, 1]
    ```
    批量大小等于 1，并且可以缩回第一个维度：
    ```json
    "dims": [256, 512, 1]
    ```
    这允许避免 TensorReshape 操作。

    如果设置了最大batch size，这个维度也可以设置为-1，例如：

    ```json
    "dims": [-1, 256, 512, 1]
    ```

    在这种情况下，维度将在运行时设置。 这启用了可变批量大小支持。

    **注意**

    最大批量大小用于引擎优化步骤，为了获得最佳性能，建议将其设置为推理时使用的实际值。

    **注意**

    ONNX 不支持可变批量大小。

    最大批量大小用于引擎优化步骤，并且
    为了获得最佳性能，建议将其设置为推理时使用的实际值。

* `max_workspace_size`（可选）- 将调整引擎的临时 GPU 内存大小。 图层算法通常需要临时工作空间。 此参数限制网络中任何层可以使用的最大大小。 如果提供的 scratch 不足，TensorRT 可能无法找到给定层的实现。 .. 注意：此参数会影响分配的 GPU 内存量和引擎性能。

* `inference_mode`（可选）- 设置是否允许 8 位和 16 位内核。 - Float16（默认）- 在引擎构建期间，将尝试启用此模式时的 fp16 内核。 - Float32 - 在引擎构建期间只允许使用 fp32 内核。

* `device_type`（可选）- 设置该层/网络将在其上执行的默认设备，GPU 或 DLA。 - GPU（默认）- 在引擎构建期间，GPU 将被设置为默认设备。 - DLA - 在引擎构建期间，DLA 引擎将用作默认设备。

* `allow_gpu_fallback`（可选）- 如果该层/网络无法在 DLA 上执行，则允许回退到 GPU。

* `force_engine_update`（可选）- 强制更新 CUDA 引擎，即使存在输入或缓存的 .plan 文件。 调试功能。

* `plugins_lib_namespace`（可选）- 使用可选命名空间初始化所有现有 TensorRT 插件并将其注册到插件注册表。 要启用插件，请设置 plugins_lib_namespace 参数。 空字符串是此参数的有效值，它指定默认的 TensorRT 命名空间：
```json
"plugins_lib_namespace": "",
```
    **注意**

    允许访问插件注册表 (initLibNvInferPlugins) 的函数应该只调用一次。 为防止从 TensorRT Inference codelet 的多个实例调用此函数，只需为单个 codelet 实例包含 Plugins Namespace 参数。

* verbose （可选）- 启用详细日志输出。 此选项启用记录 DNN 优化进度。 默认情况下禁用它以增加默认设置下日志文件的可用性。 调试功能。

球分割推理应用程序配置示例：
```json
"model_file_path": "external/ball_segmentation_model/model-9000-trimmed.uff",
"engine_file_path": "external/ball_segmentation_model/model-9000-trimmed.plan",
"input_tensor_info": [
  {
    "operation_name": "input",
    "dims": [1, 3, 256, 512]
  }
],
"output_tensor_info": [
  {
    "operation_name": "output",
    "dims": [1, 256, 512, 1]
  }
```
### SampleAccumulator小码
SampleAccumulator codelet 是一个组件，旨在缓冲来自模拟器的合成数据。 使用 Python 绑定，SampleAccumulator 可以用作训练 ML 模型的张量流数据集。

SampleAccumulator 采用一个参数，即缓冲区中要保存的最大样本数。
```json
"sample_buffer_size": 500
```

### SampleAccumulatorViewer 小码
SampleAccumulatorViewer 小码可视化在 SampleAccumulator 实例中排队的模拟数据。 它在父节点中搜索 SampleAccumulator 实例并可视化其队列缓冲区。

SampleAccumulatorViewer 采用以下参数：

* **Grid Size**：一个包含 2 个正整数的数组，用于指定在高度和宽度上堆叠的图像数量。

* **Mosaic Size**：一个包含 2 个正整数的数组，指定生成的可视化图像的高度和宽度（以像素为单位）。

* **Tick Period**：可视化更新频率。

```json
"mosaic_samples": {
  "isaac.viewers.SampleAccumulatorViewer": {
    "grid_size": [8, 8],
    "mosaic_size": [1080, 1920],
    "tick_period": "100ms"
  },
  "isaac.ml.SampleAccumulator": {
    "sample_buffer_size": 64
  }
},
```
## Tensors
在 Isaac SDK 中，Tensor 数据作为 TensorProto 的消息存储和传递，这是 Tensorflow 中使用的 numpy ndarray 数据的对应物。 ML 需要转换以适应其他数据格式，如图像。 有关示例，请参阅 [isaac.ml.ColorCameraEncoderCpu](https://docs.nvidia.com/isaac/doc/doc/component_api.html#isaac-ml-colorcameraencodercpu)。




















