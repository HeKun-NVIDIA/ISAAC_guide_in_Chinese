## 未来工厂中的搬运车

未来工厂 (FoF: Factory of the Future) 场景中的搬运车交付应用程序由 Isaac 的各种功能和特性提供支持：

* 具有逼真的物理特性的高质量、逼真的模拟环境

* 定义复杂应用程序流程的行为树

* 目标检测和 3D 姿态估计，经过全面模拟训练

* 在车道右侧行驶或走最短路径之间的选择

* 多激光雷达定位和避障

## 运行模拟器
打开一个终端窗口，导航到包含提取的 Isaac Sim 发布存档的文件夹并执行以下命令：
```bash
./builds/factory_of_the_future.x86_64 --scene Factory01
```

这将打开一个 Unity3D 窗口并加载未来工厂环境，以及机器人和一组手推车。

**注意**

此场景需要 RTX 2080 或更好的 GPU 才能以可接受的 (~30Hz) 帧速率运行。

该场景具有四种不同的相机选项，用户可以使用左右箭头键在这些选项之间切换：

* **MainCamera**：相机姿态在场景中是固定的，除非手动改变：

    1. 滚动缩放。

    2. 按住鼠标右键并移动鼠标以旋转视图。

    3. 按住鼠标中键并移动鼠标以平移视图。

* **GUICamera**：禁用外部摄像头以获得更高的 FPS。

* **FollowRobotCamera**：改变位置以跟随机器人。 不同于 ChaseRobotCamera 相机，
    该相机不随机器人旋转。

* **ChaseRobotCamera**：改变位置和旋转以追逐机器人。


## 搬运车送货申请
打开一个新的终端窗口并从 Isaac SDK 文件夹中运行以下命令：

```bash
bazel run packages/cart_delivery/apps:cart_delivery
```

要检查所有正在运行的 Isaac 节点的状态，请在 Web 浏览器中打开 Isaac Sight (http://localhost:3000)。 为获得最佳 Sight 体验，您可以禁用不需要可视化的频道。

机器人现在将自动接近并拿起手推车。 它将以预定义的姿势放下手推车，该姿势可以通过配置进行更改。

## 自动小车运输的行为树
自主小车运输的应用基于定义感知和导航任务的行为树。 此应用程序说明了通常可以部署在仓库或工厂环境中的交付场景。

应用程序定义的高级任务序列如下：

1. 机器人从它的起始位置开始。

2. 机器人移动到购物车拾取点，这是一个预定义的路点。

3. 机器人在推车下方行驶并将其抬起。

4. 机器人驾驶小车到达下车点，这是另一个预定义的路点。

5. 机器人降低推车并将其放下。 它从推车下面驶出。

6. 机器人返回其原位/空闲位置。

![](https://docs.nvidia.com/isaac/_images/behavior_tree_sequence.png)


可以在以下路径的 Isaac SDK 中找到控制上述示例应用程序的行为树子图文件：

```bash
packages/cart_delivery/apps/cart_delivery.subgraph.json
```
行为树目前承载着单一的投递任务。 但是，可以针对多次交付修改行为树。

## 导航与感知互通
感知管道的输出表示推车的估计 3D 姿态。 这个估计的姿势被写入姿势树作为目标姿势，它又被导航堆栈中的 LQR 规划器用来在购物车下行驶。 LQR Planner 也可以切换出来使用强化学习模型在车下行驶。

![](https://docs.nvidia.com/isaac/_images/perception_in_navigation_stack.png)

## 仅限自主导航申请
在没有行为树的情况下运行自主导航对于测试导航堆栈很有用。 首先，从 Isaac Sim 发布文件夹中加载未来工厂场景：

```bash
./builds/factory_of_the_future.x86_64 --scene Factory01

```
然后在 Isaac SDK 根文件夹中运行以下命令：

```bash
bazel run packages/cart_delivery/apps:navigate -- --more apps/assets/maps/virtual_factory_1.json,packages/cart_delivery/apps/navigation.config.json,packages/navsim/robots/str4.json,packages/cart_delivery/apps/pose2_planner.config.json

```
要导航到所需位置，请执行以下步骤：

1. 右键单击地图视图窗口并选择设置。

2. 在设置中，单击选择标记下拉菜单并选择“pose_as_goal”。

3. 单击添加标记。

4. 单击更新。 标记将添加到地图中。 您可能需要放大地图才能看到新标记。 机器人不会立即开始导航到标记。

5. 单击标记并将其拖动到地图上的新位置。 有关详细信息，请参阅[交互式标记](https://docs.nvidia.com/isaac/packages/sight/doc/interactiveMarkers.html#interactive-markers)。

机器人将开始导航到标记位置。

**注意**：此应用程序还具有一个 `DifferentialBaseSpeedCheck` 组件，该组件复制由可编程逻辑控制器 (PLC) 进行的速度检查，以确保机器人硬件的安全。 目前，`DifferentialBaseSpeedCheck` 仅在状态无效时停止机器人并打印到控制台，例如，由于高角速度和线速度的组合。

## 仅适用于感知
要手动控制机器人并测试感知算法，请按照以下步骤操作：

1. 从 Isaac SDK 根文件夹中运行以下命令：
```bash
bazel run packages/cart_delivery/apps:perception -- --more packages/cart_delivery/apps/detection_pose_estimation.config.json,packages/navsim/robots/str4.json

```

2. 如果可用，请使用操纵杆。 否则，请单击左侧的虚拟游戏手柄。 单击小部件上的连接到后端。 选择键盘并使用“wasd”键导航机器人。 有关详细信息，请参阅[使用 Sight 的远程操纵杆](https://docs.nvidia.com/isaac/packages/navigation/doc/virtual-gamepad.html#virtual-gamepad)。

3. 启用通道并观察 Sight 上的感知输出。

经过训练的物体检测和姿态估计模型允许机器人在仅给定 RGB 相机图像作为输入的情况下推断小车的 3D 姿态（3 DoF 平移 + 3 DoF 旋转）。 当相机距离小车 3-7 米时，检测和姿态估计模型效果最佳。

对象检测模型为检测到的小车输出一个二维轴对齐的边界框。 每次检测的感兴趣区域被裁剪并作为姿势估计模型的输入，该模型输出每个对象实例的估计旋转和平移。

**注意**

此训练步骤是可选的。 如果跳过此步骤，将使用预训练模型。

## 训练模型
### 物体检测模型（DetectNetv2）

对象检测模型是基于 ResNet-18 特征提取器的 DetectNetv2 模型。 基于真实图像的预训练模型在 Isaac Sim Unity3D 中的各种场景设置生成的模拟图像上进行了微调。 该模型将作为训练应用程序的一部分自动下载。

示例场景以 Unity3D 二进制文件的形式提供，以生成小车的随机图像以用于模型训练。 场景 7、13、14 和 15 提供了小车检测的训练数据。 例如，要启动场景 7 的场景，请从 Isaac Sim 发布文件夹运行以下命令：

```bash
./builds/factory_of_the_future.x86_64 --scene Factory01 --scenario 7

```

该模拟必须与 `generate_kitti_dataset` Isaac 应用程序一起运行，该应用程序生成一个真实数据集并以 KITTI 格式离线保存。 要收集数据集，请从 Isaac SDK 根文件夹中运行以下命令：

```bash
bazel run packages/ml/apps/generate_kitti_dataset

```

默认情况下，录制的场景存储在 `/tmp/unity3d_kitti_dataset/` 中，分别分为训练（10000 个样本）和测试（100 个样本）。 生成所有样本后，Isaac 应用程序终止。

生成数据集后，使用 NVIDIA Transfer Learning Toolkit 来训练模型。 有关 GenerateKittiDataset 应用程序和使用迁移学习工具包进行培训的更多详细信息，请参阅 Isaac SDK 文档中的“使用 DetectNetv2 进行对象检测”一章。

### 姿态估计模型（姿态CNN解码器）

姿态估计模型架构编码如下：

* 从彩色图像中裁剪出的感兴趣区域

* 边界框参数

基于这些编码，它然后将姿势估计为从连接的特征空间的回归。

所提供的姿势估计模型仅使用在 Isaac Sim Unity3D 中模拟的图像从头开始训练。 生成姿势估计训练数据的示例场景以 Unity3D 二进制文件的形式提供。 此示例需要大量 CUDA 内存，并且在同一 GPU 上运行模拟和 Isaac 应用程序可能会导致某些平台出现内存不足错误。 从 EA 发布文件夹运行以下命令：
```bash
./builds/factory_of_the_future.x86_64 --scene Factory01 --scenario 8

```
要训练模型，请从 Isaac SDK 根文件夹中运行此二进制文件以及以下 Isaac 应用程序：

```bash
bazel run packages/object_pose_estimation/apps/pose_cnn_decoder/training:pose_estimation_cnn_training

```

有关训练姿势估计模型的更多详细信息，请参阅 Isaac SDK 文档中的使用姿势 CNN 解码器进行 3D 对象姿势估计一章。

## 运行推理流程
使用经过训练的姿势估计模型，可以针对不同的目的从不同的来源进行推断。 可以在 Isaac Sight 中观察到相应的结果。

### 用真值边界框进行仿真推理

此示例从模拟中接收图像并在该图像流上运行推理流程。 结果可以在 Isaac Sight 中观察到。

1. 从 EA 发布文件夹中启动未来工厂模拟：
```bash
./builds/factory_of_the_future.x86_64 --scene Factory01 --scenario 8

```
2. 从 Isaac SDK 根文件夹中运行以下命令：
```bash
bazel run packages/object_pose_estimation/apps/pose_cnn_decoder:pose_estimation_inference_sim_groundtruth_detection

```
### 检测+姿态估计模拟推理

此示例对来自模拟的实时摄像机图像运行推车检测和姿态估计。

1. 从 EA 发布文件夹中启动未来工厂模拟：
```bash
./builds/factory_of_the_future.x86_64 --scene Factory01 --scenario 8

``` 
2. 从 Isaac SDK 根文件夹中运行以下命令：
```bash
bazel run packages/object_pose_estimation/apps/pose_cnn_decoder:detection_pose_estimation_cnn_inference_app -- --mode sim --image_channel color --intrinsics_channel color_intrinsics --config packages/object_pose_estimation/apps/pose_cnn_decoder/detection_pose_estimation_cnn_inference_dolly.config.json
```
3. 在 Web 浏览器 (http://localhost:3000) 中打开 Isaac Sight，然后在左上角的 Windows 选项卡中激活 Replay Control Panel。 在现在可见的 Replay Control Panel 中，将时间滑块移到最左侧，然后单击 START REPLAY。 推理结果将显示在预先录制的摄像机画面中。

有关姿势估计可用推理模式（如图像、日志回放和相机输入）的更多详细信息，请参阅[ Isaac SDK 文档中的“使用姿势 CNN 解码器进行 3D 对象姿势估计”](https://docs.nvidia.com/isaac/packages/object_pose_estimation/doc/pose_cnn_decoder.html#object-pose-estimation)一章。


## 检测推理参数
您可以通过 Isaac Sight 修改某些参数以在运行时调整模型的输出：每个检测都有一个关联的置信度值，置信度阈值将过滤掉所有置信度低于阈值的检测。

为了对用于对象检测的 DetectNetv2 模型的原始检测输出进行后处理，使用非最大抑制来消除单个对象实例的多次检测。 降低非最大抑制阈值以过滤掉与其他检测具有高交叉联合重叠的检测。

这些参数可以在 Isaac SDK 文件夹中的 `detection_pose_estimation.config.json` 文件中永久修改：

```bash
packages/cart_delivery/apps/detection_pose_estimation.config.json

```

它们也可以在运行时在 Isaac Sight 应用程序配置选项卡中更改。 它们可以在以下路径中找到：

```bash
detection_pose_estimation.object_detection.detection_decoder ->
isaac.detect_net.DetectNetDecoder -> confidence_threshold
                                     non_maximum_suppression_threshold
```

进行更改后，通过提交确认。

## 测绘申请
要映射环境，请执行以下步骤：

1. 打开一个新的终端窗口并从 Isaac SDK 文件夹中运行以下命令：

```bash
bazel run packages/cart_delivery/apps:gmapping -- --more packages/navsim/robots/str4.json

```

2. 在 Web 浏览器中打开 Isaac Sight (http://localhost:3000)。 为获得最佳 Sight 体验，您可以禁用不需要可视化的频道。

3. 如果可用，请使用操纵杆。 否则，请单击左侧的虚拟游戏手柄。 单击小部件上的连接到后端。 选择键盘并使用“wasd”键导航机器人。 有关详细信息，请参阅[使用 Sight 的远程操纵杆](https://docs.nvidia.com/isaac/packages/navigation/doc/virtual-gamepad.html#virtual-gamepad)。

4. 启用通道并观察在 Sight 中生成的地图。

5. 有关详细信息，请参阅 [GMapping ](https://docs.nvidia.com/isaac/apps/carter/gmapping/doc/index.html#gmapping-application)应用程序。






















